# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Parler-TTS is a lightweight text-to-speech (TTS) model that generates high-quality, natural speech with controllable voice characteristics through text descriptions. It's a reproduction of work from Stability AI and Edinburgh University.

## Architecture

The system has three main components:
1. **Text encoder**: Frozen Flan-T5 model that maps text descriptions to hidden representations
2. **Parler-TTS decoder**: Language model that generates audio tokens conditioned on encoder representations
3. **Audio codec**: DAC model from Descript that converts audio tokens back to waveforms

Key implementation details:
- Text **description** goes through encoder for cross-attention in decoder
- Text **prompt** goes through embedding layer and concatenates to decoder inputs
- Uses DAC codec instead of EnCodec for better audio quality

## Development Commands

### Code Quality
```bash
# Check code quality
make quality

# Auto-fix style issues
make style

# Individual commands
black --check .
ruff .
ruff . --fix
```

### Training
```bash
# Basic training command
accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_v1.json

# Install with training dependencies
pip install .[train]
```

### Installation
```bash
# Basic installation
pip install git+https://github.com/huggingface/parler-tts.git

# Development installation
pip install .[dev]
```

## Key Files and Directories

- `parler_tts/modeling_parler_tts.py`: Core model implementation with `ParlerTTSForConditionalGeneration`
- `parler_tts/configuration_parler_tts.py`: Model configuration classes
- `parler_tts/dac_wrapper/`: DAC audio codec integration
- `parler_tts/streamer.py`: Streaming audio generation support
- `training/`: Complete training infrastructure
- `helpers/training_configs/`: Pre-configured training recipes

## Code Standards

- Line length: 119 characters (configured in pyproject.toml)
- Code formatting: Black
- Linting: Ruff with specific ignores for line length (E501), complexity (C901)
- Python compatibility: 3.7+

## Model Usage Patterns

### Basic Inference
```python
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer

model = ParlerTTSForConditionalGeneration.from_pretrained("parler-tts/parler-tts-mini-v1")
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-v1")

# Generate audio with description and prompt
input_ids = tokenizer(description, return_tensors="pt").input_ids
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids
generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
```

### Optimization Features
- SDPA attention (default) and Flash Attention 2 support via `attn_implementation` parameter
- Model compilation with `torch.compile()` for 2-4x speedup
- Streaming generation via `ParlerTTSStreamer`
- Batch generation support

## Named Speakers
34 trained speakers available: Laura, Gary, Jon, Lea, Karen, Rick, Brenda, David, Eileen, Jordan, Mike, Yann, Joy, James, Eric, Lauren, Rose, Will, Jason, Aaron, Naomie, Alisa, Patrick, Jerry, Tina, Jenna, Bill, Tom, Carol, Barbara, Rebecca, Anna, Bruce, Emily.

## Dependencies
- transformers==4.46.1 (pinned version)
- torch, sentencepiece, descript-audio-codec
- Training: accelerate, wandb, datasets[audio], jiwer, evaluate
- Dev: black, isort, ruff

## No Testing Infrastructure
This repository does not have a test suite. When making changes, verify functionality by running inference examples and checking that audio generation works correctly.

## Project Objectives
‚Ä¢	The current Parler-TTS model feeds a text description through an encoder for cross-attention in the decoder.
‚Ä¢	We will build a model that uses a set of pre-computed vectors (the outputs of that encoder) instead of processing the description text at runtime.
‚Ä¢	The pre-computed vectors are 1024-dimensional embeddings obtained by running text tokens through the encoder. Some of these embeddings correspond closely to style attributes‚Äîe.g., gender (male, female), pitch (low, medium, high), accent (American, British, ‚Ä¶), speed (slow, moderate, quick), modulation (monotone, animated), quality (clean, noisy)‚Äîwhile others are non-attribute vectors.
‚Ä¢	Removing the encoder at inference makes the model more efficient.
‚Ä¢	Simply cross-attending these vectors in the decoder yields poor audio quality, so we will apply PEFT (parameter-efficient fine-tuning) to each vector.
‚Ä¢	Specifically, we will use LoRA adapters on all vectors, and for the attribute-related vectors we will also employ a VAE to extract mean and variance.
‚Ä¢	We will freeze all parameters of the pre-trained parler-tts-mini-v1 model except for those introduced by our PEFT modules.
‚Ä¢	We will train using a combined loss: the decoder‚Äôs codebook reconstruction loss, the VAE‚Äôs KL loss, and an orthogonality regularizer to ensure the PEFT-enhanced vectors remain distinct.
* There are six attributes in total, and the possible values for each attribute are as follows:
attribute_values = {
    "gender": ["male", "female"],
    "pitch": ["high", "low", "medium"],
    "speed": ["slowly", "quickly", "moderate"],
    "accent": [
      "Slovenia", "Chinese", "German", "Irish", "Scottish", "Russian", "Polish",
      "Canadian", "Turkish", "Czech", "Jamaica", "Italian", "Swiss", "Brazilian", "Brooklyn",
      "Finnish", "Japanese", "Filipino", "British",
      "Indian", "Australian", "Norwegian", "English",
      "Romania", "Spanish", "Croatia", "Swedish", "Colombia", "French", "American",
      "Mexican", "Portuguese", "Dominic", "Welsh", "Nigeria",
      "Chile", "Belgia", "Dutch", "Jordan", "Serbia", "Ukrainian", "Mandarin"
    ],
    "modulation": ["monoton", "animated"],
    "quality": ["clean", "noisy"]
}
## Implementation Plan
1.	Modify the existing model so that it generates speech from the pre-computed vectors rather than from the description text.
2.	Add PEFT modules to each of the pre-computed vectors.
3.	During training, the input ‚Äúpre-computed vectors‚Äù are provided as follows. For example, given the sentence
‚ÄúA female voice with American accent speaks quickly at a medium pitch and a noisy quality,‚Äù
you receive a sequence of pre-computed vectors‚Äîone for each token. The vectors corresponding to attribute tokens (female, American, quickly, medium, noisy) are trained via their respective VAE modules, while all other vectors (A, voice, with, ‚Ä¶, and, quality) are trained using LoRA. Each attribute can take on multiple values‚Äîfor instance, the gender attribute can be ‚Äúmale‚Äù or ‚Äúfemale‚Äù‚Äîand each value uses its own separate VAE weights. In other words, every possible attribute value is learned by its own dedicated VAE.
4.	From the input pre‚Äêcomputed vectors, determine which indices correspond to attribute tokens and which correspond to ‚Äúother‚Äù tokens. Then apply the matching PEFT module (VAE for attributes, LoRA for non‚Äêattributes) to each vector during training.
5.	Update the training code so that, given the pre-computed vectors, a text prompt, and paired speech data, only the PEFT parameters are fine-tuned.
6.	Write an inference script that, given a set of pre-computed vectors and input text, produces synthesized speech.

## Implementation Status ‚úÖ

### Completed Components

#### 1. Vector Organization & Loading System
- **Location**: `vectors/` directory with organized structure:
  - `vectors/gender/` - male.pt, female.pt
  - `vectors/accent/` - American.pt, British.pt, Japanese.pt, etc. (40+ accents)
  - `vectors/pitch/` - high.pt, medium.pt, low.pt
  - `vectors/speed/` - slowly.pt, quickly.pt, moderate.pt
  - `vectors/modulation/` - animated.pt, monoton.pt
  - `vectors/quality/` - clean.pt, noisy.pt
  - `vectors/nonattr_tokens/` - a.pt, voice.pt, with.pt, accent.pt, etc.

#### 2. Vector Loading & Processing (`parler_tts/vector_utils.py`)
- **VectorLoader class**: Handles style caption parsing and vector concatenation
- **Key Features**:
  - Parses natural language style captions (e.g., "female American quickly medium clean")
  - Automatically adds period (`.`) and end-of-sequence (`<_s>`) tokens for T5 compatibility
  - Distinguishes attribute vs non-attribute tokens for PEFT application
  - Loads and concatenates 1024-dim vectors in correct sequence order

```python
# Example usage
vector_loader = VectorLoader("/path/to/parler-tts")
vectors, tokens, attributes = vector_loader.get_vectors_for_caption("female American quickly")
# Returns: torch.Size([5, 1024]), ['female', 'american', 'quickly', '.', '<_s>'], {'gender': 'female', ...}
```

#### 3. Modified Model Architecture (`parler_tts/modeling_parler_tts.py`)
- **Enhanced ParlerTTSForConditionalGeneration**: Now supports precomputed vectors
- **Key Changes**:
  - Added `precomputed_vectors` parameter to forward() method
  - Added `use_precomputed_vectors` config flag
  - Vector path bypasses T5 encoder, goes directly to cross-attention
  - Maintains compatibility with original text encoder approach
  - Supports PEFT module integration (when available)

```python
# Vector-based inference
model.config.use_precomputed_vectors = True
outputs = model(
    prompt_input_ids=prompt_ids,
    precomputed_vectors=vectors.unsqueeze(0),
    attention_mask=torch.ones((1, vectors.shape[0])),
    description_tokens=[tokens]  # For PEFT module routing
)
```

#### 4. Enhanced Data Loading (`training/data.py`)
- **DataCollatorParlerTTSWithVectors**: New data collator for vector-based training
- **Features**:
  - Processes style captions to load corresponding vectors
  - Creates proper attention masks for variable-length sequences
  - Provides attribute/non-attribute indices for PEFT module application
  - Maintains compatibility with existing prompt tokenization

#### 5. PEFT Module Framework (`parler_tts/peft_modules.py`)
- **PrecomputedVectorPEFT class**: Handles vector enhancement during training
- **Planned Features** (implementation ready):
  - VAE modules for attribute tokens (mean/variance learning)
  - LoRA adapters for non-attribute tokens
  - Orthogonality regularization
  - Combined loss computation (reconstruction + KL + orthogonal)

### Verification & Testing

#### 1. Vector Processing Validation
- ‚úÖ **Token Parsing**: Style captions correctly parsed to tokens and attributes
- ‚úÖ **Vector Loading**: All attribute and non-attribute vectors load successfully
- ‚úÖ **Concatenation**: Vectors properly concatenated with correct shapes
- ‚úÖ **Index Mapping**: Attribute vs non-attribute positions correctly identified

#### 2. Model Integration Testing
- ‚úÖ **Forward Pass**: Vector-based inference works with `torch.Size([seq_len, 1024])` inputs
- ‚úÖ **Cross-Attention**: Decoder properly attends to precomputed vectors
- ‚úÖ **Attention Masks**: Padding and masking work correctly (verified with different lengths)
- ‚úÖ **Output Consistency**: 98%+ cosine similarity with original T5 encoder approach
- ‚úÖ **Batch Processing**: Supports batched inference with proper attention masking

#### 3. Sequence Alignment Verification
- ‚úÖ **Token Compatibility**: Vector tokens match T5 tokenization exactly:
  ```
  T5 tokens:     ['female', 'American', 'quickly', 'medium', 'clean', '.', '</s>']
  Vector tokens: ['female', 'american', 'quickly', 'medium', 'clean', '.', '<_s>']
  ```
- ‚úÖ **Length Matching**: Sequence lengths identical (7 tokens in above example)
- ‚úÖ **Attention Masking**: Masked positions show significant output differences (4-5 logit points)

### Usage Examples

#### Basic Vector-Based Inference
```python
from parler_tts import ParlerTTSForConditionalGeneration
from parler_tts.vector_utils import VectorLoader
from transformers import AutoTokenizer

# Setup
model = ParlerTTSForConditionalGeneration.from_pretrained("parler-tts/parler-tts-mini-v1")
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-v1")
vector_loader = VectorLoader("/path/to/parler-tts")

# Prepare inputs
style_caption = "female American quickly medium clean"
text_prompt = "Hello world"

vectors, tokens, attributes = vector_loader.get_vectors_for_caption(style_caption)
prompt_ids = tokenizer(text_prompt, return_tensors="pt").input_ids

# Configure for vector mode
model.config.use_precomputed_vectors = True
model.config.precomputed_vector_dim = 1024

# Generate
with torch.no_grad():
    generation = model.generate(
        prompt_input_ids=prompt_ids,
        precomputed_vectors=vectors.unsqueeze(0),
        attention_mask=torch.ones((1, vectors.shape[0])),
        max_new_tokens=1000
    )

# Decode audio
audio_arr = model.audio_encoder.decode(generation_reshaped).audio_values
```

#### Training Data Preparation
```python
from training.data import DataCollatorParlerTTSWithVectors
from parler_tts.vector_utils import VectorLoader

# Setup data collator
vector_loader = VectorLoader("/path/to/parler-tts")
data_collator = DataCollatorParlerTTSWithVectors(
    prompt_tokenizer=tokenizer,
    vector_loader=vector_loader
)

# Automatically processes dataset with 'description' column
# Returns batched vectors with attribute indices for PEFT
```

### Performance Benefits

#### Efficiency Gains
- ‚ö° **No T5 Encoding**: Skip expensive transformer forward pass at inference
- üöÄ **Instant Loading**: Precomputed vectors load in milliseconds
- üíæ **Memory Efficient**: No need to load/store T5 encoder during inference
- üéØ **Deterministic**: Same style description always produces identical vectors

#### Enhanced Controllability
- üéõÔ∏è **Direct Manipulation**: Edit individual attribute vectors for fine control
- üîÄ **Mix & Match**: Combine attributes from different style descriptions
- üìä **Interpretable**: Clear mapping between attributes and vector positions
- üé® **PEFT Ready**: Framework prepared for fine-grained style adaptation

### Next Steps for Full Implementation

#### 1. PEFT Module Training
- Complete VAE implementation for attribute vectors (mean/variance learning)
- Implement LoRA adapters for non-attribute vectors
- Add orthogonality regularization between enhanced vectors

#### 2. Training Pipeline Integration
- Update training script to use `DataCollatorParlerTTSWithVectors`
- Implement combined loss computation (reconstruction + KL + orthogonal)
- Add PEFT parameter isolation (freeze base model, train only PEFT)

#### 3. Advanced Features
- Vector interpolation for smooth style transitions
- Style transfer between different voice characteristics
- Dynamic vector mixing during generation

### Test Commands

```bash
# Test vector loading and concatenation
python3 show_vector_concat.py

# Test model integration and forward pass
python3 simple_vector_test.py

# Test attention mask handling
python3 check_attention_mask.py

# Compare vector vs original encoder outputs
python3 compare_outputs.py

# See detailed processing flow explanation
python3 explain_processing_flow.py
```